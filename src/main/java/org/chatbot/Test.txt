#!/usr/bin/env python3
"""
gitlog_upgrade_full_all.py
--------------------------
Parse a `git log -p` file and produce ONE merged JSONL that contains:
  - upgrade edges: Maven/Gradle deps, Version Catalog, plugins, properties, BOM/parent, wrappers, lockfiles
  - code_diff chunks for *all* code/config files (even if no bump)
  - cochange_example chunks (code/config changed in same commit as a bump)
  - release_notes snippets
  - version_ledger per (framework, group, artifact)
  - raw_diff chunks for every file (optional, ON by default) to preserve 100% of patch text

Usage:
  git -C /path/to/repo log -p -U1000000 --find-renames=99% --full-index --date=iso-strict --no-color > /tmp/repo.log

  python gitlog_upgrade_full_all.py \
    --log-file /tmp/repo.log \
    --base-prefix "frameworks/" \
    --out merged_chunks.jsonl \
    --max-cochange-per-commit -1 \
    --no-raw-diffs  # (optional) disable raw_diff if file gets too large
"""

from __future__ import annotations
import argparse, json, re, sys
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional

# ----------------- generic helpers -----------------
ANSI = re.compile(r"\x1b\[[0-9;]*m")
SEMVER = re.compile(r"^(\d+)\.(\d+)\.(\d+)(?:[+.\-][0-9A-Za-z]+)?$")

def semver_key(v: str) -> tuple[int,int,int,int]:
    m = SEMVER.match((v or "").strip())
    if not m:
        return (0,0,0,-len(v))
    return (int(m.group(1)), int(m.group(2)), int(m.group(3)), 0)

def semver_delta(a: str, b: str) -> str:
    ma, mb = SEMVER.match(a or ""), SEMVER.match(b or "")
    if not (ma and mb): return "other"
    A = (int(ma.group(1)), int(ma.group(2)), int(ma.group(3)))
    B = (int(mb.group(1)), int(mb.group(2)), int(mb.group(3)))
    if A[0] != B[0]: return "major"
    if A[1] != B[1]: return "minor"
    if A[2] != B[2]: return "patch"
    return "other"

def first_segment_after(path: str, base_prefix: str) -> str:
    path = path.lstrip("/")
    if base_prefix and base_prefix in path:
        _, after = path.split(base_prefix, 1)
        after = after.lstrip("/")
        return after.split("/", 1)[0] if after else base_prefix.strip("/").split("/")[-1]
    return path.split("/", 1)[0] if "/" in path else path

# ----------------- git log -p parser -----------------
COMMIT_LINE = re.compile(r"^commit\s+([0-9a-f]{7,40})\b", re.I)
AUTHOR_LINE = re.compile(r"^Author:\s*(.+)$", re.I)
DATE_LINE   = re.compile(r"^Date:\s*(.+)$", re.I)
DIFF_START  = re.compile(r"^diff --git a/(.+?) b/(.+)$")
HUNK_START  = re.compile(r"^@@ ")

def parse_gitlog(file_path: Path) -> List[dict]:
    """
    Returns a list of commits with minimal structure:
      {hash, author, date, diffs: [{a, b, lines: [...] }]}
    """
    txt = ANSI.sub("", file_path.read_text(encoding="utf-8", errors="ignore"))
    lines = txt.splitlines()

    commits: List[dict] = []
    cur: Optional[dict] = None
    i = 0
    while i < len(lines):
        line = lines[i]
        m = COMMIT_LINE.match(line)
        if m:
            if cur:
                commits.append(cur)
            cur = {"hash": m.group(1), "author": "", "date": "", "diffs": []}
            i += 1
            # headers until blank/diff
            while i < len(lines):
                l = lines[i]
                if DIFF_START.match(l) or l.strip() == "":
                    break
                am = AUTHOR_LINE.match(l)
                dm = DATE_LINE.match(l)
                if am: cur["author"] = am.group(1).strip()
                if dm: cur["date"]   = dm.group(1).strip()
                i += 1
            continue

        if cur:
            dm = DIFF_START.match(line)
            if dm:
                a_path, b_path = dm.group(1), dm.group(2)
                diff = {"a": a_path, "b": b_path, "lines": []}
                i += 1
                while i < len(lines):
                    l2 = lines[i]
                    if COMMIT_LINE.match(l2) or DIFF_START.match(l2):
                        break
                    diff["lines"].append(l2)
                    i += 1
                cur["diffs"].append(diff)
                continue
        i += 1

    if cur:
        commits.append(cur)
    return commits

# ----------------- FULL diff snippet helpers (no filtering/truncation) -----------------
def full_diff_text(lines: List[str], max_lines: int = -1) -> str:
    if max_lines is not None and max_lines > 0:
        return "\n".join(lines[:max_lines])
    return "\n".join(lines)

# ----------------- detectors: core (Gradle/Maven deps) -----------------
BUILD_FILES = {"pom.xml", "build.gradle", "build.gradle.kts"}

# Gradle deps like "group:artifact:version"
GR_COORD_RE = re.compile(r'["\']([A-Za-z0-9_.\-]+:[A-Za-z0-9_.\-]+):([A-Za-z0-9+_.\-]+)["\']')

# Maven inlined dependency triplet (simple)
POM_G_RE = re.compile(r"<groupId>\s*([^<]+)\s*</groupId>", re.I)
POM_A_RE = re.compile(r"<artifactId>\s*([^<]+)\s*</artifactId>", re.I)
POM_V_RE = re.compile(r"<version>\s*([^<]+)\s*</version>", re.I)

def detect_gradle_bumps(lines: List[str]) -> List[Tuple[str,str,str]]:
    minus, plus = {}, {}
    for ln in lines:
        if not ln: continue
        if ln[0] in "+-":
            for m in GR_COORD_RE.finditer(ln[1:]):
                ga, ver = m.group(1), m.group(2)
                if ln[0] == "-": minus[ga] = ver
                else: plus[ga] = ver
    return [(ga, minus[ga], plus[ga]) for ga in sorted(set(minus)&set(plus)) if minus[ga]!=plus[ga]]

def detect_pom_bumps(lines: List[str]) -> List[Tuple[str,str,str]]:
    cur_g = cur_a = None
    pending_from: Optional[str] = None
    out: List[Tuple[str,str,str]] = []
    for ln in lines:
        if not ln: continue
        head, body = (ln[0], ln[1:]) if ln[0] in "+- " else (None, ln)
        mg = POM_G_RE.search(body); ma = POM_A_RE.search(body)
        if mg: cur_g = mg.group(1).strip()
        if ma: cur_a = ma.group(1).strip()
        mv = POM_V_RE.search(body)
        if head == "-" and mv:
            pending_from = mv.group(1).strip()
        elif head == "+" and mv and pending_from:
            to_v = mv.group(1).strip()
            if cur_g and cur_a and pending_from != to_v:
                out.append((f"{cur_g}:{cur_a}", pending_from, to_v))
            pending_from = None
        if HUNK_START.match(ln):
            pending_from = None
    return out

# ----------------- detectors: Catalog/Plugins/Properties/BOM/Parent/Wrappers/Lockfile -----------------
TOML_FILE_RE   = re.compile(r".*/libs\.versions\.toml$")
TOML_VER_KV    = re.compile(r'^([+\-])\s*([A-Za-z0-9_.-]+)\s*=\s*"(.*?)"\s*$')  # in [versions]
TOML_LIB_KV    = re.compile(r'([A-Za-z0-9_.-]+)\s*=\s*\{\s*module\s*=\s*"(.*?)"[^}]*version\.ref\s*=\s*"(.*?)"')
GR_PLUGIN_RE   = re.compile(r'^[+\-]\s*id\(["\']([^"\']+)["\']\)\s*version\s*["\']([^"\']+)["\']')
PROP_LINE_RE   = re.compile(r'^([+\-])\s*([A-Za-z0-9_.-]+)\s*=\s*([^\s#]+)\s*$')  # gradle.properties style
MVN_PROP_RE    = re.compile(r'^[+\-].*?<([A-Za-z0-9_.-]+)\.version>\s*([^<\s]+)\s*</\1\.version>')
MVN_PARENT_RE  = re.compile(r'^[+\-].*?<parent>.*?<groupId>\s*([^<]+)\s*</groupId>.*?<artifactId>\s*([^<]+)\s*</artifactId>.*?<version>\s*([^<]+)\s*</version>', re.S)
MVN_BOM_RE     = re.compile(r'^[+\-].*?<dependency>.*?<groupId>\s*([^<]+)\s*</groupId>.*?<artifactId>\s*([^<]+)\s*</artifactId>.*?<version>\s*([^<]+)\s*</version>.*?<type>\s*pom\s*</type>.*?<scope>\s*import\s*</scope>', re.S)
WRAPPER_GR_RE  = re.compile(r'^[+\-].*?distributionUrl=.*?gradle-([0-9][0-9.\-a-z]+)-')
WRAPPER_MVN_RE = re.compile(r'^[+\-].*?distributionUrl=.*?apache-maven-([0-9][0-9.\-a-z]+)-')

LOCKFILE_RE    = re.compile(r".*gradle\.lockfile$")
LOCK_LINE_RE   = re.compile(r'^[+\-]\s*([A-Za-z0-9_.\-]+):([A-Za-z0-9_.\-]+):([A-Za-z0-9+_.\-]+)')

def detect_toml_catalog_bumps(lines: List[str]) -> List[Tuple[str,str,str]]:
    minus, plus = {}, {}
    in_versions = False
    for ln in lines:
        s = ln.strip()
        if s.startswith("[versions]"): in_versions = True; continue
        if s.startswith("[") and not s.startswith("[versions]"): in_versions = False
        if not in_versions: continue
        m = TOML_VER_KV.match(ln)
        if not m: continue
        sign, key, val = m.groups()
        if sign == "-": minus[key] = val
        elif sign == "+": plus[key] = val
    if not minus or not plus:
        return []
    libs: Dict[str, Tuple[str,str]] = {}
    for ln in lines:
        m = TOML_LIB_KV.search(ln)
        if m:
            alias, module, vref = m.groups()
            libs[alias] = (module, vref)
    by_ref: Dict[str, List[str]] = {}
    for alias,(module,vref) in libs.items():
        by_ref.setdefault(vref, []).append(module)
    out = []
    for key in set(minus) & set(plus):
        if minus[key] != plus[key]:
            for module in by_ref.get(key, []):
                out.append((module, minus[key], plus[key]))
    return out

def detect_gradle_plugin_bumps(lines: List[str]) -> List[Tuple[str,str,str]]:
    minus, plus = {}, {}
    for ln in lines:
        m = GR_PLUGIN_RE.match(ln)
        if not m: continue
        pid, ver = m.groups()
        if ln.lstrip().startswith("-"): minus[pid] = ver
        elif ln.lstrip().startswith("+"): plus[pid] = ver
    return [(pid, minus[pid], plus[pid]) for pid in set(minus)&set(plus) if minus[pid]!=plus[pid]]

def detect_gradle_properties_bumps(lines: List[str]) -> List[Tuple[str,str,str]]:
    minus, plus = {}, {}
    for ln in lines:
        m = PROP_LINE_RE.match(ln)
        if not m: continue
        sign, key, val = m.groups()
        if sign == "-": minus[key] = val
        else: plus[key] = val
    return [(k, minus[k], plus[k]) for k in set(minus)&set(plus) if minus[k]!=plus[k]]

def detect_maven_property_bumps(lines: List[str]) -> List[Tuple[str,str,str]]:
    minus, plus = {}, {}
    for ln in lines:
        m = MVN_PROP_RE.match(ln)
        if not m: continue
        sign = ln[0]
        pname, val = m.group(1)+".version", m.group(2)
        if sign == "-": minus[pname] = val
        else: plus[pname] = val
    return [(k, minus[k], plus[k]) for k in set(minus)&set(plus) if minus[k]!=plus[k]]

def detect_maven_parent_bump(lines: List[str]) -> List[Tuple[str,str,str]]:
    minus = plus = None
    for ln in lines:
        m = MVN_PARENT_RE.match(ln)
        if m:
            ga = f"{m.group(1)}:{m.group(2)}"; ver = m.group(3)
            if ln[0] == "-": minus = (ga, ver)
            elif ln[0] == "+": plus = (ga, ver)
    return [(minus[0], minus[1], plus[1])] if minus and plus and minus[1]!=plus[1] else []

def detect_maven_bom_import_bumps(lines: List[str]) -> List[Tuple[str,str,str]]:
    minus, plus = {}, {}
    for ln in lines:
        m = MVN_BOM_RE.match(ln)
        if not m: continue
        ga = f"{m.group(1)}:{m.group(2)}"; ver = m.group(3)
        if ln[0] == "-": minus[ga] = ver
        else: plus[ga]   = ver
    return [(ga, minus[ga], plus[ga]) for ga in set(minus)&set(plus) if minus[ga]!=plus[ga]]

def detect_wrapper_bumps(lines: List[str]) -> List[Tuple[str,str,str,str]]:
    gradle_from = gradle_to = maven_from = maven_to = None
    for ln in lines:
        mg = WRAPPER_GR_RE.match(ln)
        if mg:
            if ln[0] == "-": gradle_from = mg.group(1)
            elif ln[0] == "+": gradle_to = mg.group(1)
        mm = WRAPPER_MVN_RE.match(ln)
        if mm:
            if ln[0] == "-": maven_from = mm.group(1)
            elif ln[0] == "+": maven_to = mm.group(1)
    out = []
    if gradle_from and gradle_to and gradle_from!=gradle_to:
        out.append(("gradle", gradle_from, gradle_to, "gradle-wrapper.properties"))
    if maven_from and maven_to and maven_from!=maven_to:
        out.append(("maven", maven_from, maven_to, "maven-wrapper.properties"))
    return out

def detect_lockfile_bumps(lines: List[str]) -> List[Tuple[str,str,str]]:
    minus, plus = {}, {}
    for ln in lines:
        m = LOCK_LINE_RE.match(ln)
        if not m: continue
        ga = f"{m.group(1)}:{m.group(2)}"; ver = m.group(3)
        if ln[0] == "-": minus[ga] = ver
        else: plus[ga] = ver
    return [(ga, minus[ga], plus[ga]) for ga in set(minus)&set(plus) if minus[ga]!=plus[ga]]

# ----------------- co-change, code & notes -----------------
CODE_FILES_RE = re.compile(r".*\.(java|kt|kts|scala|groovy|jsp|jj|proto)$", re.I)
CONF_FILES_RE = re.compile(r"(application.*\.(ya?ml|properties)|logback.*\.xml|log4j2\.xml|.*\.xml)$", re.I)
NOTES_FILES_RE = re.compile(r"(CHANGELOG\.md|MIGRATION\.md|UPGRADING\.md|RELEASE_NOTES\.md)$", re.I)

def extract_notes_snippet(diff_lines: List[str], max_lines:int=200) -> str:
    kept, out = 0, []
    for ln in diff_lines:
        if ln.startswith("@@"):
            out.append(ln); continue
        if ln and ln[0] in "+-":
            if re.search(r"(?i)breaking|deprecat|migrat|upgrade|change|fix|security|CVE|\b(v|version)\s*\d", ln):
                out.append(ln)
                kept += 1
                if kept >= max_lines: break
    return "\n".join(out)

# ----------------- data classes (unified shape) -----------------
@dataclass
class Chunk:
    external_id: str
    language: str
    symbol_kind: str
    is_recipe: bool
    is_config: bool
    is_example: bool
    content: str
    metadata: Dict[str, Any]
    # embedding omitted (set to null in JSONL)

def make_edge_chunk(fw: str, chash: str, author: str, date: str,
                    path: str, ga: str, v1: str, v2: str,
                    kind: str="upgrade", full_lines: Optional[List[str]]=None) -> Chunk:
    group, artifact = ga.split(":", 1)
    delta = semver_delta(v1, v2)
    header = (
        f"[Upgrade]\n"
        f"Framework: {fw}\n"
        f"Commit: {chash}\n"
        f"Author: {author}\n"
        f"Date: {date}\n"
        f"File: {path}\n"
        f"Change: {ga} {v1} → {v2} ({delta})\n\n"
    )
    body = header + (full_diff_text(full_lines) if full_lines else "(diff omitted)")
    meta = {
        "framework_folder": fw,
        "group": group, "artifact": artifact,
        "from_version": v1, "to_version": v2, "semver_delta": delta,
        "commit": chash, "author": author, "date": date,
        "source_path": path,
        "tags": ["upgrade"], "keywords": [fw, group, artifact, v1, v2, kind]
    }
    return Chunk(
        external_id=path or chash,
        language="text",
        symbol_kind=kind,
        is_recipe=True, is_config=False, is_example=False,
        content=body,
        metadata=meta
    )

def make_property_chunk(fw: str, chash: str, author: str, date: str,
                        path: str, prop: str, v1: str, v2: str, maven: bool=False) -> Chunk:
    header = (
        f"[Version Property]\nFramework: {fw}\nCommit: {chash}\nAuthor: {author}\nDate: {date}\n"
        f"File: {path}\nChange: {prop} {v1} → {v2}\n\n"
    )
    meta = {
        "framework_folder": fw,
        "group": "property", "artifact": prop,
        "from_version": v1, "to_version": v2,
        "commit": chash, "author": author, "date": date,
        "source_path": path,
        "tags": ["version_property","upgrade"], "keywords": [fw, prop, v1, v2, "maven" if maven else "gradle"]
    }
    return Chunk(
        external_id=path or chash,
        language="text",
        symbol_kind="version_property",
        is_recipe=True, is_config=False, is_example=False,
        content=header + "(diff omitted)",
        metadata=meta
    )

def make_plugin_chunk(fw: str, chash: str, author: str, date: str,
                      path: str, plugin_id: str, v1: str, v2: str, full_lines: Optional[List[str]]) -> Chunk:
    header = (
        f"[Plugin Upgrade]\nFramework: {fw}\nCommit: {chash}\nAuthor: {author}\nDate: {date}\n"
        f"File: {path}\nChange: {plugin_id} {v1} → {v2}\n\n"
    )
    meta = {
        "framework_folder": fw,
        "group": "plugin", "artifact": plugin_id,
        "from_version": v1, "to_version": v2,
        "commit": chash, "author": author, "date": date,
        "source_path": path,
        "tags": ["upgrade","plugin"], "keywords": [fw, plugin_id, v1, v2]
    }
    return Chunk(
        external_id=path or chash,
        language="text",
        symbol_kind="plugin_upgrade",
        is_recipe=True, is_config=False, is_example=False,
        content=header + (full_diff_text(full_lines) if full_lines else "(diff omitted)"),
        metadata=meta
    )

def make_tool_chunk(fw: str, chash: str, author: str, date: str,
                    path: str, tool: str, v1: str, v2: str, full_lines: Optional[List[str]]) -> Chunk:
    header = (
        f"[Tool Upgrade]\nFramework: {fw}\nCommit: {chash}\nAuthor: {author}\nDate: {date}\n"
        f"File: {path}\nChange: {tool} {v1} → {v2}\n\n"
    )
    meta = {
        "framework_folder": fw,
        "group": "tool", "artifact": tool,
        "from_version": v1, "to_version": v2,
        "commit": chash, "author": author, "date": date,
        "source_path": path,
        "tags": ["upgrade","tool"], "keywords": [fw, tool, v1, v2]
    }
    return Chunk(
        external_id=path or chash,
        language="text",
        symbol_kind="tool_upgrade",
        is_recipe=True, is_config=False, is_example=False,
        content=header + (full_diff_text(full_lines) if full_lines else "(diff omitted)"),
        metadata=meta
    )

def make_cochange_chunk(fw: str, chash: str, author: str, date: str,
                        path: str, ga: str, v1: str, v2: str,
                        full_lines: List[str], is_config: bool) -> Chunk:
    group, artifact = ga.split(":",1)
    header = (
        f"[Co-change]\nFramework: {fw}\nCommit: {chash}\nAuthor: {author}\nDate: {date}\n"
        f"File: {path}\nRelated bump: {ga} {v1} → {v2}\n\n"
    )
    meta = {
        "framework_folder": fw,
        "group": group, "artifact": artifact,
        "from_version": v1, "to_version": v2,
        "commit": chash, "author": author, "date": date,
        "source_path": path,
        "tags": ["upgrade","cochange"] + (["config"] if is_config else []),
        "keywords": [fw, group, artifact, v1, v2, Path(path).suffix]
    }
    body = header + full_diff_text(full_lines)
    return Chunk(
        external_id=path or chash,
        language="text",
        symbol_kind="cochange_example",
        is_recipe=True, is_config=is_config, is_example=True,
        content=body, metadata=meta
    )

def make_notes_chunk(fw: str, chash: str, author: str, date: str,
                     path: str, snippet: str) -> Chunk:
    header = (
        f"[Release Notes]\nFramework: {fw}\nCommit: {chash}\nAuthor: {author}\nDate: {date}\nFile: {path}\n\n"
    )
    meta = {
        "framework_folder": fw,
        "group": "docs", "artifact": Path(path).name,
        "commit": chash, "author": author, "date": date,
        "source_path": path,
        "tags": ["release_notes","upgrade"], "keywords": [fw, "breaking", "migration", "deprecated"]
    }
    return Chunk(
        external_id=path or chash,
        language="text",
        symbol_kind="release_notes",
        is_recipe=False, is_config=False, is_example=False,
        content=header + (snippet or ""),
        metadata=meta
    )

def make_ledger_chunk(fw: str, group: str, artifact: str,
                      versions: List[str], latest: str, latest_date: Optional[str]) -> Chunk:
    content = (
        f"[Versions]\nFramework: {fw}\nArtifact: {group}:{artifact}\n"
        f"Observed: {', '.join(versions)}\nLatest: {latest}" + (f" ({latest_date})" if latest_date else "")
    )
    meta = {
        "framework_folder": fw,
        "group": group, "artifact": artifact,
        "versions": versions, "latest": latest, "latest_date": latest_date,
        "tags": ["versions","upgrade"], "keywords": [fw, group, artifact, "latest","versions"]
    }
    return Chunk(
        external_id=f"versions/{fw}/{group}:{artifact}",
        language="text",
        symbol_kind="version_ledger",
        is_recipe=False, is_config=False, is_example=False,
        content=content, metadata=meta
    )

def make_code_diff_chunk(fw, chash, author, date, path, lines) -> Chunk:
    return Chunk(
        external_id=path or chash,
        language="text",
        symbol_kind="code_diff",
        is_recipe=False, is_config=bool(CONF_FILES_RE.match(path)), is_example=False,
        content=(f"[Code Diff]\nFramework: {fw}\nCommit: {chash}\nAuthor: {author}\nDate: {date}\nFile: {path}\n\n"
                 + full_diff_text(lines)),
        metadata={
            "framework_folder": fw,
            "group": "code", "artifact": Path(path).name,
            "commit": chash, "author": author, "date": date,
            "source_path": path,
            "tags": ["code_diff"], "keywords": [fw, "code", Path(path).suffix]
        }
    )

def make_raw_diff_chunk(fw, chash, author, date, path, lines) -> Chunk:
    return Chunk(
        external_id=path or chash,
        language="text",
        symbol_kind="raw_diff",
        is_recipe=False, is_config=False, is_example=False,
        content=(f"[Raw Diff]\nFramework: {fw}\nCommit: {chash}\nAuthor: {author}\nDate: {date}\nFile: {path}\n\n"
                 + full_diff_text(lines)),
        metadata={
            "framework_folder": fw,
            "group": "raw", "artifact": Path(path).name,
            "commit": chash, "author": author, "date": date,
            "source_path": path,
            "tags": ["raw_diff"], "keywords": [fw, "raw", "diff"]
        }
    )

# ----------------- build all chunks -----------------
def build_chunks(commits: List[dict], base_prefix: str,
                 max_cochange_per_commit:int = -1,
                 include_raw_diffs: bool = True) -> List[Chunk]:
    edges: List[Chunk] = []
    plugin_up: List[Chunk] = []
    props: List[Chunk] = []
    tools: List[Chunk] = []
    notes: List[Chunk] = []
    cochg: List[Chunk] = []
    code_diffs: List[Chunk] = []
    raw_diffs: List[Chunk] = []

    # remember bumps per commit for co-change association
    bumps_by_commit: Dict[str, List[Tuple[str,str,str,str]]] = {}  # commit -> list of (fw, ga, from, to)

    for c in commits:
        chash, author, date = c.get("hash",""), c.get("author",""), c.get("date","")
        produced_co = 0

        for d in c["diffs"]:
            bpath = d["b"]; name = Path(bpath).name
            fw = first_segment_after(bpath, base_prefix)
            lines = d["lines"]

            # Always capture raw diffs (if enabled)
            if include_raw_diffs:
                raw_diffs.append(make_raw_diff_chunk(fw, chash, author, date, bpath, lines))

            # Capture code/config diffs regardless of bumps
            if CODE_FILES_RE.match(bpath) or CONF_FILES_RE.match(bpath):
                code_diffs.append(make_code_diff_chunk(fw, chash, author, date, bpath, lines))

            # --- primary upgrade sources ---
            bump_list: List[Tuple[str,str,str]] = []

            if name in ("build.gradle","build.gradle.kts"):
                bump_list += detect_gradle_bumps(lines)
                for pid, v1, v2 in detect_gradle_plugin_bumps(lines):
                    plugin_up.append(make_plugin_chunk(fw, chash, author, date, bpath, pid, v1, v2, lines))

            elif name == "pom.xml":
                bump_list += detect_pom_bumps(lines)
                for prop, v1, v2 in detect_maven_property_bumps(lines):
                    props.append(make_property_chunk(fw, chash, author, date, bpath, prop, v1, v2, maven=True))
                for ga, v1, v2 in detect_maven_parent_bump(lines):
                    edges.append(make_edge_chunk(fw, chash, author, date, bpath, ga, v1, v2, kind="parent_upgrade", full_lines=lines))
                for ga, v1, v2 in detect_maven_bom_import_bumps(lines):
                    edges.append(make_edge_chunk(fw, chash, author, date, bpath, ga, v1, v2, kind="bom_upgrade", full_lines=lines))

            elif TOML_FILE_RE.match(bpath):
                for ga, v1, v2 in detect_toml_catalog_bumps(lines):
                    bump_list.append((ga, v1, v2))

            elif name == "gradle.properties":
                for prop, v1, v2 in detect_gradle_properties_bumps(lines):
                    props.append(make_property_chunk(fw, chash, author, date, bpath, prop, v1, v2, maven=False))

            elif name in ("settings.gradle","settings.gradle.kts"):
                for pid, v1, v2 in detect_gradle_plugin_bumps(lines):
                    plugin_up.append(make_plugin_chunk(fw, chash, author, date, bpath, pid, v1, v2, lines))

            elif name in ("gradle-wrapper.properties","maven-wrapper.properties"):
                for tool, v1, v2, file_tag in detect_wrapper_bumps(lines):
                    tools.append(make_tool_chunk(fw, chash, author, date, bpath, tool, v1, v2, lines))

            elif LOCKFILE_RE.match(bpath):
                for ga, v1, v2 in detect_lockfile_bumps(lines):
                    edges.append(make_edge_chunk(fw, chash, author, date, bpath, ga, v1, v2, kind="lock_update", full_lines=lines))

            # build-file dependency bumps → edges (with full diff)
            for (ga, v1, v2) in bump_list:
                edges.append(make_edge_chunk(fw, chash, author, date, bpath, ga, v1, v2, kind="upgrade", full_lines=lines))
                bumps_by_commit.setdefault(chash, []).append((fw, ga, v1, v2))

            # release/migration notes snippets
            if NOTES_FILES_RE.search(name):
                sn = extract_notes_snippet(lines)
                if sn:
                    notes.append(make_notes_chunk(fw, chash, author, date, bpath, sn))

        # ---- co-change across files in this commit (full diffs, no cap unless >0) ----
        if chash in bumps_by_commit:
            bumps = bumps_by_commit[chash]
            for d in c["diffs"]:
                bpath = d["b"]; name = Path(bpath).name
                if name in BUILD_FILES:
                    continue
                if not (CODE_FILES_RE.match(bpath) or CONF_FILES_RE.match(bpath)):
                    continue
                fw = first_segment_after(bpath, base_prefix)
                lines = d["lines"]
                for (fw0, ga, v1, v2) in bumps:
                    if fw0 != fw:
                        continue
                    cochg.append(make_cochange_chunk(fw, chash, author, date, bpath, ga, v1, v2, lines, bool(CONF_FILES_RE.match(bpath))))
                    produced_co += 1
                    if max_cochange_per_commit > 0 and produced_co >= max_cochange_per_commit:
                        break
                if max_cochange_per_commit > 0 and produced_co >= max_cochange_per_commit:
                    break

    # ---- version ledgers from edges (and BOM/parent/lock_update too) ----
    ledgers = build_ledgers_from_edges(edges)

    # Merge all chunks
    all_chunks: List[Chunk] = []
    all_chunks.extend(edges)
    all_chunks.extend(plugin_up)
    all_chunks.extend(props)
    all_chunks.extend(tools)
    all_chunks.extend(notes)
    all_chunks.extend(cochg)
    all_chunks.extend(code_diffs)
    if include_raw_diffs:
        all_chunks.extend(raw_diffs)
    all_chunks.extend(ledgers)

    # Deduplicate by (symbol_kind, path, commit, group, artifact)
    seen = set()
    uniq: List[Chunk] = []
    for ch in all_chunks:
        meta = ch.metadata or {}
        key = (ch.symbol_kind, ch.external_id, meta.get("commit",""), meta.get("group",""), meta.get("artifact",""))
        if key in seen:
            continue
        seen.add(key)
        uniq.append(ch)

    return uniq

def build_ledgers_from_edges(edge_chunks: List[Chunk]) -> List[Chunk]:
    # Collect per (framework, group, artifact)
    buckets: Dict[Tuple[str,str,str], Dict[str, Optional[str]]] = {}
    for ch in edge_chunks:
        md = ch.metadata or {}
        fw = md.get("framework_folder","")
        g  = md.get("group","")
        a  = md.get("artifact","")
        if not (fw and g and a): 
            continue
        fv = md.get("from_version"); tv = md.get("to_version")
        d  = md.get("date")
        key = (fw,g,a)
        if key not in buckets:
            buckets[key] = {}
        if fv: buckets[key].setdefault(fv, None)
        if tv: buckets[key][tv] = buckets[key].get(tv) or d

    ledgers: List[Chunk] = []
    for (fw,g,a), vers in buckets.items():
        ordered = sorted(vers.keys(), key=semver_key)
        if not ordered:
            continue
        latest = ordered[-1]
        latest_date = vers.get(latest)
        ledgers.append(make_ledger_chunk(fw, g, a, ordered, latest, latest_date))
    return ledgers

# ----------------- CLI -----------------
def main():
    ap = argparse.ArgumentParser(description="Parse git log -p and emit ONE JSONL with edges/ledgers/cochanges/code_diffs/notes/raw_diffs (full diffs, no truncation).")
    ap.add_argument("--log-file", required=True, help="Path to file produced by `git log -p` (use -U1000000, --date=iso-strict, --no-color).")
    ap.add_argument("--base-prefix", default="frameworks/", help="Prefix to infer framework folder (e.g., 'frameworks/').")
    ap.add_argument("--out", default="merged_chunks.jsonl", help="Output JSONL path (single file with all chunks).")
    ap.add_argument("--max-cochange-per-commit", type=int, default=-1, help="-1 = unlimited; otherwise cap per commit.")
    ap.add_argument("--no-raw-diffs", action="store_true", help="If set, do NOT emit raw_diff chunks for every file.")
    args = ap.parse_args()

    commits = parse_gitlog(Path(args.log_file))
    chunks = build_chunks(
        commits,
        args.base_prefix,
        max_cochange_per_commit=args.max_cochange_per_commit,
        include_raw_diffs=(not args.no_raw_diffs)
    )
    with open(args.out, "w", encoding="utf-8") as f:
        for ch in chunks:
            obj = asdict(ch)
            obj["embedding"] = None  # let your ingest/backfill add vectors (or keep NULL)
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")

    print(f"[ok] commits: {len(commits)} | chunks written: {len(chunks)} → {args.out}")

if __name__ == "__main__":
    main()
