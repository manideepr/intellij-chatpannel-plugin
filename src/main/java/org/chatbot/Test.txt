#!/usr/bin/env python3
"""
Two-table RAG with PGVector (collections + chunks) + Auto-Routed Retrieval

Features
- Tables: kb_collections (registry), kb_chunks (vectors) with FK collection_id
- Ingest a single collection OR every child dir under a parent root (each child = one collection)
- AST/fallback Java chunking + config/build chunkers + README→recipe extraction
- Hybrid retrieval (vector + trigram) with boosts (recipes/config/examples)
- Auto-routing: discover top-N relevant collections for a query, then retrieve within them
- Optional query expansion via synonyms file (yaml/json)

Install
  pip install javalang pyyaml psycopg2-binary pgvector numpy tiktoken lxml

Optional (OpenAI embeddings)
  pip install openai
  export EMBED_PROVIDER=openai
  export OPENAI_API_KEY=...
  # optional: EMBED_MODEL=text-embedding-3-small (1536) or *-large (3072)

Postgres env
  PGHOST, PGPORT, PGDATABASE, PGUSER, PGPASSWORD

Usage
  # Ingest ONE collection
  python rag_pgvector_kb_autoroute.py ingest \
    --root /path/to/repo \
    --collection code --kb-type code --version 1.3.0 \
    --embedding-dim 1536

  # Ingest every child dir under a parent; each child becomes a collection
  python rag_pgvector_kb_autoroute.py ingest-parent \
    --parent-root /path/to/frameworks \
    --kb-type code --embedding-dim 1536 \
    --name-prefix fw- \
    --name-map /path/to/collections_map.yaml   # optional

  # Auto-search (no framework passed). It will pick top collections itself.
  python rag_pgvector_kb_autoroute.py search-auto \
    --query "I want to set up a DynamoDB table" \
    --k 8 --top-collections 5 \
    --embedding-dim 1536 \
    --synonyms /path/to/synonyms.yaml          # optional
"""
from __future__ import annotations

import argparse
import hashlib
import json
import os
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import numpy as np
import psycopg2
from psycopg2.extras import execute_values, Json
from pgvector.psycopg2 import register_vector

# Optional deps
try:
    import javalang  # type: ignore
except Exception:
    javalang = None
try:
    import yaml  # type: ignore
except Exception:
    yaml = None
try:
    import tiktoken  # type: ignore
except Exception:
    tiktoken = None

# -------------------------------
# Constants & helpers
# -------------------------------
JAVA_EXTS = {".java"}
CONFIG_PROPS_EXTS = {".properties"}
CONFIG_YAML_EXTS = {".yml", ".yaml"}
BUILD_GRADLE = {"build.gradle", "build.gradle.kts"}
POM_FILES = {"pom.xml"}

DEFAULT_MAX_TOKENS = 600
DEFAULT_HARD_CAP = 800

HEADER_TEMPLATE = (
    "[Collection: {collection}]\n"
    "[Version: {version}]\n"
    "[Package: {package}]\n"
    "[Class: {class_name}]\n"
    "[Method: {method_name}]\n"
    "[Signature: {signature}]\n"
    "[Imports: {imports}]\n"
)

IGNORE_DIRS = {".git", ".idea", ".vscode", "target", "build", "out", "node_modules"}

@dataclass
class Chunk:
    external_id: str
    language: str
    symbol_kind: str  # method|config|build|recipe|doc
    is_recipe: bool
    is_config: bool
    is_example: bool
    package: Optional[str]
    class_name: Optional[str]
    method_name: Optional[str]
    signature: Optional[str]
    start_line: Optional[int]
    end_line: Optional[int]
    tokens: int
    content: str
    raw_code: Optional[str]
    meta: Dict
    hash_key: str

def build_tokenizer():
    if tiktoken is not None:
        try:
            enc = tiktoken.get_encoding("cl100k_base")
            return lambda s: len(enc.encode(s))
        except Exception:
            pass
    return lambda s: max(1, len(re.findall(r"\w+|\S", s)))

TOK_COUNT = build_tokenizer()

def _read_text(p: Path) -> str:
    return p.read_text(encoding="utf-8", errors="ignore")

# -------------------------------
# Java parsing & chunking
# -------------------------------
def _extract_javadoc_above(lines: List[str], from_line_1_based: int) -> str:
    i = from_line_1_based - 2
    while i >= 0 and lines[i].strip() == "":
        i -= 1
    if i < 0:
        return ""
    if "*/" in lines[i]:
        end = i
        while i >= 0:
            if "/**" in lines[i]:
                start = i
                return "\n".join(lines[start:end+1])
            i -= 1
    return ""

def _find_block_span(lines: List[str], start_line_1: int) -> Tuple[int, int]:
    start_idx = start_line_1 - 1
    text_from = "\n".join(lines[start_idx:])
    brace_pos = text_from.find("{")
    if brace_pos == -1:
        return start_line_1, start_line_1
    depth = 0; i = 0
    for ch in text_from[brace_pos:]:
        if ch == "{": depth += 1
        elif ch == "}":
            depth -= 1
            if depth == 0: break
        i += 1
    end_line = start_line_1 + (text_from[: brace_pos + i].count("\n"))
    return start_line_1, max(start_line_1, end_line)

def _format_signature(md) -> str:
    try:
        rt = getattr(md, 'return_type', None)
        rt_str = getattr(rt, 'name', None) if rt else ('void' if getattr(md, 'constructor', False) else 'void')
        params = []
        for p in getattr(md, 'parameters', []) or []:
            ptype = getattr(p.type, 'name', str(p.type))
            pname = getattr(p, 'name', 'arg')
            dims = '[]' * getattr(p, 'dimensions', 0)
            params.append(f"{ptype}{dims} {pname}")
        name = getattr(md, 'name', '<init>')
        mods = ' '.join(sorted(getattr(md, 'modifiers', []) or []))
        return f"{mods} {rt_str} {name}({', '.join(params)})".strip()
    except Exception:
        return str(md)

METHOD_DECL_RE = re.compile(r"^(\s*(public|private|protected|static|final|synchronized|abstract)\s+)*\s*[\w\<\>\[\]]+\s+[\w]+\s*\([^\)]*\)\s*\{", re.M)

def _java_ast_chunks(path: Path, collection: str, version: str) -> List[Chunk]:
    if javalang is None:
        return []
    src = _read_text(path); lines = src.splitlines(); out: List[Chunk] = []
    try:
        tree = javalang.parse.parse(src)
    except Exception:
        return out
    pkg = getattr(getattr(tree, 'package', None), 'name', None)
    imports = [imp.path for imp in getattr(tree, 'imports', [])]
    for type_decl in getattr(tree, 'types', []) or []:
        class_name = getattr(type_decl, 'name', None)
        for md in getattr(type_decl, 'methods', []) or []:
            pos = getattr(md, 'position', None); line = pos.line if pos else 1
            jdoc = _extract_javadoc_above(lines, line)
            signature = _format_signature(md); method_name = getattr(md, 'name', None)
            s_line, e_line = _find_block_span(lines, line)
            raw_code = "\n".join(lines[s_line - 1:e_line])
            header = HEADER_TEMPLATE.format(
                collection=collection, version=version,
                package=pkg or "", class_name=class_name or "",
                method_name=method_name or "", signature=signature,
                imports=", ".join(imports[:24])
            )
            content = "\n".join([header] + ([jdoc] if jdoc else []) + [raw_code])
            tokens = TOK_COUNT(content)
            if tokens > DEFAULT_HARD_CAP:
                blocks = re.split(r"\n\s*\n+", raw_code)
                current: List[str] = []; assembled: List[str] = []
                for b in blocks:
                    cand = "\n".join(current + [b])
                    cand_content = "\n".join([header] + ([jdoc] if jdoc else []) + [cand])
                    if TOK_COUNT(cand_content) > DEFAULT_MAX_TOKENS and current:
                        assembled.append("\n\n".join(current))
                        current = current[-1:] + [b]  # tiny overlap
                    else:
                        current.append(b)
                if current: assembled.append("\n\n".join(current))
                for part in assembled:
                    part_content = "\n".join([header] + ([jdoc] if jdoc else []) + [part])
                    out.append(_make_chunk(
                        external_id=str(path), language="java", symbol_kind="method",
                        is_recipe=False, is_config=False, is_example=False,
                        package=pkg, class_name=class_name, method_name=method_name, signature=signature,
                        start_line=s_line, end_line=e_line, tokens=TOK_COUNT(part_content),
                        content=part_content, raw_code=part, meta={}
                    ))
            else:
                out.append(_make_chunk(
                    external_id=str(path), language="java", symbol_kind="method",
                    is_recipe=False, is_config=False, is_example=False,
                    package=pkg, class_name=class_name, method_name=method_name, signature=signature,
                    start_line=s_line, end_line=e_line, tokens=tokens,
                    content=content, raw_code=raw_code, meta={}
                ))
    return out

def _java_fallback_chunks(path: Path, collection: str, version: str) -> List[Chunk]:
    src = _read_text(path); lines = src.splitlines(); out: List[Chunk] = []
    pkg_m = re.search(r"^\s*package\s+([\w\.]+)\s*;", src, re.M)
    pkg = pkg_m.group(1) if pkg_m else None
    imports = re.findall(r"^\s*import\s+([\w\.\*]+)\s*;", src, re.M)
    cls_m = re.search(r"\b(class|interface|enum)\s+(\w+)", src)
    class_name = cls_m.group(2) if cls_m else None
    for m in METHOD_DECL_RE.finditer(src):
        start = m.start(); start_line = src.count("\n", 0, start) + 1
        s_line, e_line = _find_block_span(lines, start_line)
        raw_code = "\n".join(lines[s_line - 1:e_line])
        header = HEADER_TEMPLATE.format(
            collection=collection, version=version, package=pkg or "",
            class_name=class_name or "", method_name="", signature="(fallback)",
            imports=", ".join(imports[:24])
        )
        content = header + "\n" + raw_code
        out.append(_make_chunk(
            external_id=str(path), language="java", symbol_kind="method",
            is_recipe=False, is_config=False, is_example=False,
            package=pkg, class_name=class_name, method_name=None, signature=None,
            start_line=s_line, end_line=e_line, tokens=TOK_COUNT(content),
            content=content, raw_code=raw_code, meta={}
        ))
    return out

# -------------------------------
# Config / build chunkers
# -------------------------------
def _flatten_yaml(d: Dict, prefix: str = "") -> List[Tuple[str, str]]:
    pairs: List[Tuple[str, str]] = []
    if isinstance(d, dict):
        for k, v in d.items():
            key = f"{prefix}{k}" if not prefix else f"{prefix}.{k}"
            pairs.extend(_flatten_yaml(v, key))
    elif isinstance(d, list):
        for i, v in enumerate(d):
            key = f"{prefix}[{i}]"
            pairs.extend(_flatten_yaml(v, key))
    else:
        pairs.append((prefix, str(d)))
    return pairs

def _chunk_properties(path: Path, collection: str, version: str, batch: int = 6) -> List[Chunk]:
    lines = _read_text(path).splitlines(); kvs: List[Tuple[str, str]] = []
    for ln in lines:
        ln = ln.strip()
        if not ln or ln.startswith("#"): continue
        if "=" in ln: k, v = ln.split("=", 1); kvs.append((k.strip(), v.strip()))
        elif ":" in ln: k, v = ln.split(":", 1); kvs.append((k.strip(), v.strip()))
    out: List[Chunk] = []
    for i in range(0, len(kvs), batch):
        group = kvs[i:i+batch]
        body = "\n".join(f"{k}={v}" for k, v in group)
        header = f"[Collection: {collection}]\n[Version: {version}]\n[Config File: {path.name}]\n[Path: {path}]\n"
        content = header + "\n" + body
        out.append(_make_chunk(
            external_id=str(path), language="properties", symbol_kind="config",
            is_recipe=False, is_config=True, is_example=False,
            package=None, class_name=None, method_name=None, signature=None,
            start_line=None, end_line=None, tokens=TOK_COUNT(content),
            content=content, raw_code=body, meta={"config_keys":[k for k,_ in group]}
        ))
    return out

def _chunk_yaml(path: Path, collection: str, version: str, batch: int = 8) -> List[Chunk]:
    if yaml is None: return []
    try:
        data = yaml.safe_load(_read_text(path))
    except Exception:
        return []
    pairs = _flatten_yaml(data) if isinstance(data, (dict, list)) else []
    out: List[Chunk] = []
    for i in range(0, len(pairs), batch):
        group = pairs[i:i+batch]; lines = [f"{k}: {v}" for k, v in group]; body = "\n".join(lines)
        header = f"[Collection: {collection}]\n[Version: {version}]\n[Config File: {path.name}]\n[Path: {path}]\n"
        content = header + "\n" + body
        out.append(_make_chunk(
            external_id=str(path), language="yaml", symbol_kind="config",
            is_recipe=False, is_config=True, is_example=False,
            package=None, class_name=None, method_name=None, signature=None,
            start_line=None, end_line=None, tokens=TOK_COUNT(content),
            content=content, raw_code=body, meta={"config_keys":[k for k,_ in group]}
        ))
    return out

def _chunk_gradle(path: Path, collection: str, version: str) -> List[Chunk]:
    text = _read_text(path)
    deps = [ln.strip() for ln in text.splitlines()
            if re.search(r"\b(implementation|api|compileOnly|runtimeOnly)\b\s*\(\"", ln)]
    if not deps: return []
    body = "\n".join(deps)
    header = f"[Collection: {collection}]\n[Version: {version}]\n[Build File: {path.name}]\n[Path: {path}]\n"
    content = header + "\n" + body
    return [ _make_chunk(
        external_id=str(path), language="gradle", symbol_kind="build",
        is_recipe=False, is_config=False, is_example=False,
        package=None, class_name=None, method_name=None, signature=None,
        start_line=None, end_line=None, tokens=TOK_COUNT(content),
        content=content, raw_code=body, meta={"artifacts_guess": deps[:5]}
    ) ]

def _chunk_pom(path: Path, collection: str, version: str) -> List[Chunk]:
    try:
        from lxml import etree
        xml = etree.fromstring(_read_text(path).encode("utf-8"))
        deps = xml.xpath("//dependency")
        lines = []
        for d in deps:
            g = d.findtext("groupId") or ""
            a = d.findtext("artifactId") or ""
            v = d.findtext("version") or ""
            scope = d.findtext("scope") or ""
            lines.append(f"{g}:{a}:{v} {('['+scope+']') if scope else ''}")
        if not lines: return []
        body = "\n".join(lines)
    except Exception:
        return []
    header = f"[Collection: {collection}]\n[Version: {version}]\n[Build File: {path.name}]\n[Path: {path}]\n"
    content = header + "\n" + body
    return [ _make_chunk(
        external_id=str(path), language="maven", symbol_kind="build",
        is_recipe=False, is_config=False, is_example=False,
        package=None, class_name=None, method_name=None, signature=None,
        start_line=None, end_line=None, tokens=TOK_COUNT(content),
        content=content, raw_code=body, meta={"artifacts": lines}
    ) ]

# -------------------------------
# README → recipe chunks
# -------------------------------
HEAD_RE = re.compile(r"^#{1,4}\s+(.*)$", re.M)
FENCE_RE = re.compile(r"```(\w+)?\n(.*?)\n```", re.S)

def _build_recipe_chunks_from_readme(readme: Path, collection: str, version: str, example_name: str) -> List[Chunk]:
    text = _read_text(readme)
    sections = [(m.group(1).strip(), m.start()) for m in HEAD_RE.finditer(text)]
    if not sections: sections = [("Instructions", 0)]
    sections.append(("__END__", len(text)))
    spans = [(sections[i][0], text[sections[i][1]:sections[i+1][1]]) for i in range(len(sections)-1)]
    out: List[Chunk] = []
    for title, body in spans:
        if title == "__END__": continue
        fences = FENCE_RE.findall(body)
        code_bits, cmds, artifacts, cfg_keys = [], [], [], []
        for lang, code in fences:
            lang = (lang or "").lower()
            if lang in {"java","kotlin","kt"}: code_bits.append(code.strip())
            elif lang in {"yaml","yml","properties"}:
                code_bits.append(code.strip())
                cfg_keys += re.findall(r"^[a-zA-Z0-9_.-]+\s*[:=]", code, re.M)
            elif lang in {"bash","sh"}: cmds += [l.strip() for l in code.splitlines() if l.strip()]
            elif lang in {"xml","gradle","groovy","kts"}: artifacts.append(code.strip())
        if not code_bits and not cmds and not artifacts: continue
        header = (
            f"[Recipe: {title}]\n"
            f"[Example: {example_name}]\n"
            f"[Collection: {collection}]\n"
            f"[Run: {cmds[0] if cmds else ''}]\n"
            f"[ConfigKeys: {', '.join(k.split(':')[0].split('=')[0] for k in cfg_keys[:6])}]\n"
        )
        body_min = "\n\n".join(code_bits[:3])
        content = header + "\n" + body_min
        meta = {
            "example_name": example_name,
            "commands": cmds[:5],
            "artifacts": artifacts[:5],
            "config_keys": [k.split(':')[0].split('=')[0] for k in cfg_keys[:10]],
            "readme_path": str(readme),
            "recipe_title": title,
        }
        out.append(_make_chunk(
            external_id=str(readme), language="markdown", symbol_kind="recipe",
            is_recipe=True, is_config=False, is_example=True,
            package=None, class_name=None, method_name=None, signature=None,
            start_line=None, end_line=None, tokens=TOK_COUNT(content),
            content=content, raw_code=body_min, meta=meta
        ))
    return out

def _find_example_readmes(root: Path) -> List[Path]:
    cands = []
    for p in root.rglob("README.md"):
        parent = p.parent.name.lower()
        if parent in {"examples","example","samples","sample"} or "example" in parent or "sample" in parent:
            cands.append(p)
    if not cands:
        cands = list(root.rglob("README.md"))
    return cands

# -------------------------------
# Chunk factory
# -------------------------------
def _make_chunk(
    external_id: str, language: str, symbol_kind: str,
    is_recipe: bool, is_config: bool, is_example: bool,
    package: Optional[str], class_name: Optional[str], method_name: Optional[str], signature: Optional[str],
    start_line: Optional[int], end_line: Optional[int], tokens: int,
    content: str, raw_code: Optional[str], meta: Dict
) -> Chunk:
    hash_src = f"{external_id}|{start_line}|{end_line}|{hashlib.sha1(content.encode()).hexdigest()}"
    return Chunk(
        external_id=external_id, language=language, symbol_kind=symbol_kind,
        is_recipe=is_recipe, is_config=is_config, is_example=is_example,
        package=package, class_name=class_name, method_name=method_name, signature=signature,
        start_line=start_line, end_line=end_line, tokens=tokens,
        content=content, raw_code=raw_code, meta=meta, hash_key=hash_src
    )

# -------------------------------
# Embeddings
# -------------------------------
def get_embedder(embedding_dim: int):
    provider = os.getenv("EMBED_PROVIDER", "none").lower()
    if provider == "openai":
        try:
            from openai import OpenAI  # type: ignore
        except Exception:
            raise RuntimeError("Install openai>=1.0.0 for OPENAI provider")
        model = os.getenv("EMBED_MODEL", "text-embedding-3-small")
        client = OpenAI()
        def embed(text: str) -> np.ndarray:
            if len(text) > 200_000: text = text[:200_000]
            out = client.embeddings.create(model=model, input=text)
            vec = np.array(out.data[0].embedding, dtype=np.float32)
            if vec.shape[0] != embedding_dim:
                raise RuntimeError(f"Embedding dim {vec.shape[0]} != table dim {embedding_dim}")
            return vec
        return embed
    elif provider == "none":
        def embed(_text: str) -> np.ndarray:
            return np.zeros((embedding_dim,), dtype=np.float32)
        return embed
    else:
        raise RuntimeError(f"Unsupported EMBED_PROVIDER={provider}")

# -------------------------------
# Postgres schema & SQL
# -------------------------------
DDL = """
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;

CREATE TABLE IF NOT EXISTS kb_collections (
  id             BIGSERIAL PRIMARY KEY,
  name           TEXT UNIQUE NOT NULL,
  kb_type        TEXT NOT NULL,
  description    TEXT,
  embedding_dim  INT NOT NULL DEFAULT {dim},
  distance       TEXT NOT NULL DEFAULT 'cosine',
  metadata       JSONB DEFAULT '{{}}'::jsonb,
  created_at     TIMESTAMPTZ NOT NULL DEFAULT now()
);

CREATE TABLE IF NOT EXISTS kb_chunks (
  id             BIGSERIAL PRIMARY KEY,
  collection_id  BIGINT NOT NULL REFERENCES kb_collections(id) ON DELETE CASCADE,
  external_id    TEXT,
  language       TEXT,
  symbol_kind    TEXT,
  is_recipe      BOOLEAN DEFAULT FALSE,
  is_config      BOOLEAN DEFAULT FALSE,
  is_example     BOOLEAN DEFAULT FALSE,
  package        TEXT,
  class_name     TEXT,
  method_name    TEXT,
  signature      TEXT,
  start_line     INT,
  end_line       INT,
  tokens         INT,
  content        TEXT NOT NULL,
  raw_code       TEXT,
  metadata       JSONB DEFAULT '{{}}'::jsonb,
  embedding      VECTOR({dim}),
  hash_key       TEXT UNIQUE
);

CREATE INDEX IF NOT EXISTS kb_chunks_embedding_hnsw
  ON kb_chunks USING hnsw (embedding vector_cosine_ops);
CREATE INDEX IF NOT EXISTS kb_chunks_content_trgm
  ON kb_chunks USING gin (content gin_trgm_ops);
CREATE INDEX IF NOT EXISTS kb_chunks_collection ON kb_chunks(collection_id);
CREATE INDEX IF NOT EXISTS kb_chunks_collection_kind ON kb_chunks(collection_id, symbol_kind);
"""

INSERT_SQL = """
INSERT INTO kb_chunks
  (collection_id, external_id, language, symbol_kind, is_recipe, is_config, is_example,
   package, class_name, method_name, signature, start_line, end_line, tokens,
   content, raw_code, metadata, embedding, hash_key)
VALUES %s
ON CONFLICT (hash_key) DO NOTHING
"""

# Stage 1: discover candidate collections
CANDIDATE_COLLECTIONS_SQL = """
WITH vec AS (
  SELECT collection_id, 1 - (embedding <=> %s) AS vscore
  FROM kb_chunks
  ORDER BY embedding <=> %s
  LIMIT 1000
),
lex AS (
  SELECT collection_id, GREATEST(similarity(content, %s), 0) AS lscore
  FROM kb_chunks
  WHERE content % %s
  ORDER BY lscore DESC
  LIMIT 1000
),
unioned AS (
  SELECT collection_id, vscore, 0::float AS lscore FROM vec
  UNION ALL
  SELECT collection_id, 0::float, lscore FROM lex
),
agg AS (
  SELECT collection_id,
         MAX(vscore) AS vscore,
         MAX(lscore) AS lscore,
         (0.7*MAX(vscore) + 0.3*MAX(lscore)) AS fused
  FROM unioned
  GROUP BY collection_id
)
SELECT collection_id
FROM agg
ORDER BY fused DESC
LIMIT %s;
"""

# Stage 2: actual hybrid retrieve within chosen collections
HYBRID_WITHIN_SQL = """
WITH vec AS (
  SELECT id, 1 - (embedding <=> %s) AS vscore
  FROM kb_chunks
  WHERE collection_id = ANY(%s)
  ORDER BY embedding <=> %s
  LIMIT 200
),
lex AS (
  SELECT id, GREATEST(similarity(content, %s), 0) AS lscore
  FROM kb_chunks
  WHERE collection_id = ANY(%s) AND content % %s
  ORDER BY lscore DESC
  LIMIT 200
),
unioned AS (
  SELECT id, vscore, 0::float AS lscore FROM vec
  UNION ALL
  SELECT id, 0::float, lscore FROM lex
),
agg AS (
  SELECT id,
         MAX(vscore) AS vscore,
         MAX(lscore) AS lscore,
         (0.7*MAX(vscore) + 0.3*MAX(lscore)
          + 0.10*MAX(CASE WHEN is_example THEN 1 ELSE 0 END)
          + 0.10*MAX(CASE WHEN is_recipe THEN 1 ELSE 0 END)
          + 0.08*MAX(CASE WHEN is_config THEN 1 ELSE 0 END)) AS fused
  FROM unioned
  JOIN kb_chunks USING (id)
  GROUP BY id
)
SELECT c.id, c.collection_id, c.external_id, c.symbol_kind, c.is_recipe, c.is_config, c.is_example,
       a.vscore, a.lscore, a.fused, c.content
FROM agg a
JOIN kb_chunks c USING (id)
ORDER BY a.fused DESC
LIMIT %s;
"""

VEC_ONLY_WITHIN_SQL = """
SELECT id, collection_id, external_id, symbol_kind, is_recipe, is_config, is_example,
       1 - (embedding <=> %s) AS vscore, content
FROM kb_chunks
WHERE collection_id = ANY(%s)
ORDER BY embedding <=> %s
LIMIT %s;
"""

# -------------------------------
# DB helpers
# -------------------------------
def _connect():
    conn = psycopg2.connect(
        host=os.getenv("PGHOST", "localhost"),
        port=int(os.getenv("PGPORT", "5432")),
        dbname=os.getenv("PGDATABASE"),
        user=os.getenv("PGUSER"),
        password=os.getenv("PGPASSWORD"),
    )
    register_vector(conn)
    return conn

def get_or_create_collection(conn, name: str, kb_type: str, dim: int, description: str = "", metadata: dict | None = None) -> int:
    metadata = metadata or {}
    with conn.cursor() as cur:
        cur.execute("SELECT id, embedding_dim FROM kb_collections WHERE name=%s", (name,))
        row = cur.fetchone()
        if row:
            cid, existing_dim = row
            if existing_dim != dim:
                print(f"[WARN] Collection '{name}' dim={existing_dim} (ignoring requested {dim}).", file=sys.stderr)
            return cid
        cur.execute("""
            INSERT INTO kb_collections (name, kb_type, description, embedding_dim, metadata)
            VALUES (%s,%s,%s,%s,%s)
            RETURNING id
        """, (name, kb_type, description, dim, Json(metadata)))
        cid = cur.fetchone()[0]
    conn.commit()
    return cid

# -------------------------------
# Build chunks (one repo)
# -------------------------------
def _should_skip_dir(p: Path) -> bool:
    return p.name in IGNORE_DIRS or p.name.startswith(".")

def build_chunks(root: Path, collection: str, version: str, include_examples: bool = True) -> List[Chunk]:
    chunks: List[Chunk] = []
    for path in root.rglob("*"):
        if path.is_dir():
            if _should_skip_dir(path):  # skip noisy dirs
                continue
            else:
                continue
        # files
        ext = path.suffix.lower(); name = path.name
        try:
            if ext in JAVA_EXTS:
                ast_chunks = _java_ast_chunks(path, collection, version)
                chunks.extend(ast_chunks if ast_chunks else _java_fallback_chunks(path, collection, version))
            elif ext in CONFIG_PROPS_EXTS:
                chunks.extend(_chunk_properties(path, collection, version))
            elif ext in CONFIG_YAML_EXTS:
                chunks.extend(_chunk_yaml(path, collection, version))
            elif name in BUILD_GRADLE:
                chunks.extend(_chunk_gradle(path, collection, version))
            elif name in POM_FILES:
                chunks.extend(_chunk_pom(path, collection, version))
        except Exception as e:
            print(f"[WARN] Failed: {path}: {e}", file=sys.stderr)
    if include_examples:
        for readme in _find_example_readmes(root):
            # avoid READMEs in ignored dirs
            if any(seg in IGNORE_DIRS for seg in readme.parts): continue
            example_name = readme.parent.name
            chunks.extend(_build_recipe_chunks_from_readme(readme, collection, version, example_name))
    return chunks

# -------------------------------
# Ingest (one collection)
# -------------------------------
def ingest_one(root: Path, collection: str, kb_type: str, version: str, embedding_dim: int):
    conn = _connect()
    with conn.cursor() as cur:
        cur.execute(DDL.format(dim=embedding_dim))
    conn.commit()

    cid = get_or_create_collection(conn, collection, kb_type, embedding_dim)
    embed = get_embedder(embedding_dim)

    chunks = build_chunks(root, collection, version, include_examples=True)
    print(f"[{collection}] built {len(chunks)} chunks", file=sys.stderr)

    batch = 200; idx = 0
    with conn.cursor() as cur:
        while idx < len(chunks):
            part = chunks[idx: idx+batch]
            rows = []
            for c in part:
                vec = embed(c.content).tolist()
                rows.append((
                    cid, c.external_id, c.language, c.symbol_kind, c.is_recipe, c.is_config, c.is_example,
                    c.package, c.class_name, c.method_name, c.signature, c.start_line, c.end_line, c.tokens,
                    c.content, c.raw_code, Json(c.meta), vec, c.hash_key
                ))
            execute_values(cur, INSERT_SQL, rows, page_size=batch)
            conn.commit(); idx += batch
            print(f"[{collection}] inserted {min(idx, len(chunks))}/{len(chunks)}", file=sys.stderr)
    conn.close()

# -------------------------------
# Ingest every child directory as its own collection
# -------------------------------
def _load_name_map(path: Optional[str]) -> Dict[str, str]:
    if not path: return {}
    p = Path(path)
    try:
        if p.suffix.lower() in {".yaml", ".yml"} and yaml:
            return yaml.safe_load(p.read_text(encoding="utf-8")) or {}
        else:
            return json.loads(p.read_text(encoding="utf-8"))
    except Exception as e:
        print(f"[WARN] Could not load name map {path}: {e}", file=sys.stderr)
        return {}

def ingest_parent(parent_root: Path, kb_type: str, version: str, embedding_dim: int,
                  name_prefix: str = "", name_map_path: Optional[str] = None):
    name_map = _load_name_map(name_map_path)
    for child in sorted(parent_root.iterdir()):
        if not child.is_dir(): continue
        if _should_skip_dir(child): continue
        collection = name_map.get(child.name, f"{name_prefix}{child.name}")
        print(f"==> Ingesting collection '{collection}' from {child}", file=sys.stderr)
        ingest_one(child, collection, kb_type, version, embedding_dim)

# -------------------------------
# Search (auto-routed)
# -------------------------------
def _embed_query_vector(embedding_dim: int, text: str) -> List[float]:
    embed = get_embedder(embedding_dim)
    return embed(text).tolist()

def _expand_query(query: str, synonyms_path: Optional[str]) -> str:
    if not synonyms_path: return query
    p = Path(synonyms_path); expansion = []
    try:
        if p.suffix.lower() in {".yaml", ".yml"} and yaml:
            data = yaml.safe_load(p.read_text(encoding="utf-8")) or {}
        else:
            data = json.loads(p.read_text(encoding="utf-8"))
        for key, extra in (data or {}).items():
            if key.lower() in query.lower():
                if isinstance(extra, str): expansion.append(extra)
                elif isinstance(extra, list): expansion.extend(extra)
    except Exception as e:
        print(f"[WARN] Failed to load synonyms {synonyms_path}: {e}", file=sys.stderr)
    if expansion:
        return query + " " + " ".join(expansion)
    return query

def auto_route_search(query: str, k: int, top_collections: int, embedding_dim: int,
                      vec_only: bool = False, synonyms_path: Optional[str] = None):
    conn = _connect()
    q_expanded = _expand_query(query, synonyms_path)
    emb = _embed_query_vector(embedding_dim, q_expanded)
    with conn.cursor() as cur:
        # 1) Candidate collections
        cur.execute(CANDIDATE_COLLECTIONS_SQL, (emb, emb, q_expanded, q_expanded, top_collections))
        ids = [r[0] for r in cur.fetchall()]
        if not ids:
            conn.close()
            return []
        # 2) Retrieve within selected collections
        if vec_only:
            cur.execute(VEC_ONLY_WITHIN_SQL, (emb, ids, emb, k))
            rows = cur.fetchall()
        else:
            cur.execute(HYBRID_WITHIN_SQL, (emb, ids, emb, q_expanded, ids, q_expanded, k))
            rows = cur.fetchall()
    conn.close()
    return rows, ids

# -------------------------------
# CLI
# -------------------------------
def main():
    ap = argparse.ArgumentParser(description="PGVector RAG (kb_collections + kb_chunks) with auto-routed retrieval")
    sub = ap.add_subparsers(dest="cmd", required=True)

    ap_ing = sub.add_parser("ingest", help="Ingest one collection from --root")
    ap_ing.add_argument("--root", required=True)
    ap_ing.add_argument("--collection", required=True)
    ap_ing.add_argument("--kb-type", default="code")
    ap_ing.add_argument("--version", default="0.0.0")
    ap_ing.add_argument("--embedding-dim", type=int, default=int(os.getenv("EMBEDDING_DIM", "1536")))

    ap_ip = sub.add_parser("ingest-parent", help="Ingest each child directory as its own collection")
    ap_ip.add_argument("--parent-root", required=True)
    ap_ip.add_argument("--kb-type", default="code")
    ap_ip.add_argument("--version", default="0.0.0")
    ap_ip.add_argument("--embedding-dim", type=int, default=int(os.getenv("EMBEDDING_DIM", "1536")))
    ap_ip.add_argument("--name-prefix", default="")
    ap_ip.add_argument("--name-map", default=None, help="YAML/JSON mapping: folder_name -> collection_name")

    ap_sa = sub.add_parser("search-auto", help="Auto-route query to top-N collections and retrieve")
    ap_sa.add_argument("--query", required=True)
    ap_sa.add_argument("--k", type=int, default=8)
    ap_sa.add_argument("--top-collections", type=int, default=5)
    ap_sa.add_argument("--embedding-dim", type=int, default=int(os.getenv("EMBEDDING_DIM", "1536")))
    ap_sa.add_argument("--vec-only", action="store_true")
    ap_sa.add_argument("--synonyms", default=None, help="Optional YAML/JSON for query expansion")

    args = ap.parse_args()

    if args.cmd == "ingest":
        root = Path(args.root).expanduser().resolve()
        if not root.exists():
            print(f"Root not found: {root}", file=sys.stderr); sys.exit(2)
        # Ensure schema before ingest
        conn = _connect()
        with conn.cursor() as cur: cur.execute(DDL.format(dim=args.embedding_dim))
        conn.commit(); conn.close()
        ingest_one(root, args.collection, args.kb_type, args.version, args.embedding_dim)

    elif args.cmd == "ingest-parent":
        parent = Path(args.parent_root).expanduser().resolve()
        if not parent.exists():
            print(f"Parent root not found: {parent}", file=sys.stderr); sys.exit(2)
        conn = _connect()
        with conn.cursor() as cur: cur.execute(DDL.format(dim=args.embedding_dim))
        conn.commit(); conn.close()
        ingest_parent(parent, args.kb_type, args.version, args.embedding_dim,
                      name_prefix=args.name_prefix, name_map_path=args.name_map)

    elif args.cmd == "search-auto":
        rows, ids = auto_route_search(args.query, args.k, args.top_collections, args.embedding_dim,
                                      vec_only=args.vec_only, synonyms_path=args.synonyms)
        print(f"[auto] candidate collections: {ids}")
        for r in rows:
            if len(r) == 10:   # vec-only
                (_id, coll_id, ext, kind, is_rec, is_cfg, is_ex, v, content) = r[0], r[1], r[2], r[3], r[4], r[5], r[6], r[7], r[8]
                print(f"[c{coll_id}] {kind:6} | {ext} | vscore={v:.3f}")
            else:              # hybrid
                (_id, coll_id, ext, kind, is_rec, is_cfg, is_ex, v, l, f, content) = r
                print(f"[c{coll_id}] {kind:6} | {ext} | score={f:.3f}")

if __name__ == "__main__":
    main()
