#!/usr/bin/env python3
"""
kb_ingest_with_chunking.py
--------------------------
Create kb tables, upsert collection, and insert chunks idempotently with
**pre-embedding character-based chunking** to avoid 400s from your embedding API.

Install:
  pip install psycopg2-binary pgvector requests

Env:
  PGHOST, PGPORT, PGDATABASE, PGUSER, PGPASSWORD

Usage examples:

  # Insert from a JSONL and call your embedder, chunking content to <= 7000 chars with 300 overlap
  python kb_ingest_with_chunking.py \
    --collection-name git-log \
    --kb-type git_log \
    --embedding-dim 1024 \
    --input-jsonl merged_chunks.jsonl \
    --embed-endpoint https://your-embedder/api/embeddings \
    --max-embed-chars 7000 \
    --overlap 300

  # Skip embeddings for now (stores NULL) but still chunk rows in DB
  python kb_ingest_with_chunking.py \
    --collection-name code-java \
    --kb-type code \
    --embedding-dim 1024 \
    --input-jsonl chunks.jsonl \
    --no-embed \
    --max-embed-chars 7000 \
    --overlap 300
"""

from __future__ import annotations
import argparse, json, os, re, sys, time, hashlib, math
from typing import Any, Dict, List, Optional, Tuple

import requests
import psycopg2
from psycopg2.extras import Json, execute_values
from pgvector.psycopg2 import register_vector

# ------------------------------ DDL ------------------------------
DDL = """
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;

CREATE TABLE IF NOT EXISTS kb_collections (
  id BIGSERIAL PRIMARY KEY,
  name TEXT UNIQUE NOT NULL,
  kb_type TEXT NOT NULL,                -- e.g. 'code', 'git_log', 'confluence'
  description TEXT,
  embedding_dim INT NOT NULL,
  distance TEXT NOT NULL DEFAULT 'cosine',
  metadata JSONB DEFAULT '{}'::jsonb,
  created_at TIMESTAMPTZ DEFAULT now()
);

CREATE TABLE IF NOT EXISTS kb_chunks (
  id BIGSERIAL PRIMARY KEY,
  collection_id BIGINT NOT NULL REFERENCES kb_collections(id) ON DELETE CASCADE,
  external_id TEXT,                     -- path/url/commit/file id (part-aware)
  language TEXT,
  symbol_kind TEXT,                     -- 'code','upgrade','version_ledger','raw_diff',...
  is_recipe BOOLEAN DEFAULT FALSE,
  is_config BOOLEAN DEFAULT FALSE,
  is_example BOOLEAN DEFAULT FALSE,
  package TEXT, class_name TEXT, method_name TEXT, signature TEXT,
  start_line INT, end_line INT, tokens INT,
  content TEXT NOT NULL,                -- what you search over
  raw_code TEXT,                        -- optional full code
  metadata JSONB DEFAULT '{}'::jsonb,   -- tags/keywords/parts metadata, etc.
  embedding VECTOR(%(dim)s),
  hash_key TEXT UNIQUE                  -- for idempotent upserts
);

CREATE INDEX IF NOT EXISTS kb_chunks_embedding_hnsw ON kb_chunks USING hnsw (embedding vector_cosine_ops);
CREATE INDEX IF NOT EXISTS kb_chunks_content_trgm   ON kb_chunks USING gin  (content gin_trgm_ops);
CREATE INDEX IF NOT EXISTS kb_chunks_collection     ON kb_chunks(collection_id);
"""

# ------------------------------ DB helpers ------------------------------
def connect():
    conn = psycopg2.connect(
        host=os.getenv("PGHOST","localhost"),
        port=int(os.getenv("PGPORT","5432")),
        dbname=os.getenv("PGDATABASE"),
        user=os.getenv("PGUSER"),
        password=os.getenv("PGPASSWORD"),
    )
    register_vector(conn)
    return conn

def ensure_schema(conn, embedding_dim: int):
    with conn.cursor() as cur:
        cur.execute(DDL, {"dim": embedding_dim})
    conn.commit()

def ensure_collection(conn, *, name: str, kb_type: str, embedding_dim: int,
                      description: Optional[str] = None,
                      metadata: Optional[Dict[str, Any]] = None) -> int:
    metadata = metadata or {}
    with conn.cursor() as cur:
        cur.execute("""
            INSERT INTO kb_collections(name, kb_type, description, embedding_dim, metadata)
            VALUES (%s,%s,%s,%s,%s)
            ON CONFLICT (name) DO NOTHING;
        """, (name, kb_type, description, embedding_dim, Json(metadata)))
        cur.execute("SELECT id, embedding_dim FROM kb_collections WHERE name=%s", (name,))
        row = cur.fetchone()
        if not row:
            raise RuntimeError("Failed to read/create collection")
        cid, dim = row
        if dim != embedding_dim:
            raise ValueError(f"Collection '{name}' embedding_dim={dim} != expected {embedding_dim}. "
                             f"Use a new collection or re-create with the desired dimension.")
        return cid

# ------------------------------ Hashing ------------------------------
def _hash_key(fields: List[str]) -> str:
    h = hashlib.sha256("|".join([f or "" for f in fields]).encode("utf-8")).hexdigest()
    return h

def _hash_key_for_row(collection_id: int, r: Dict[str, Any], part_index: Optional[int] = None) -> str:
    md = r.get("metadata") or {}
    part_str = f"#{part_index}" if part_index is not None else ""
    fields = [
        str(collection_id),
        (r.get("external_id") or "") + part_str,
        r.get("symbol_kind") or "",
        md.get("commit",""),
        (md.get("from_version","") + "->" + md.get("to_version","")) if (md.get("from_version") or md.get("to_version")) else "",
        (r.get("content") or "")[:128],
    ]
    return _hash_key(fields)

# ------------------------------ Chunking ------------------------------
def _greedy_chunk_text(text: str, max_chars: int, overlap: int = 0) -> List[str]:
    """
    Greedy character-based chunker with soft boundaries:
    - Prefer splits on double newlines, then single newlines, else hard cut.
    - Adds `overlap` chars of lookback context (except for first chunk).
    """
    if text is None:
        return [""]
    if max_chars <= 0 or len(text) <= max_chars:
        return [text]

    chunks: List[str] = []
    i = 0
    L = len(text)

    # Helper: find the best split <= end
    def find_split(start: int, end: int) -> int:
        window = text[start:end]
        # try double newline
        idx = window.rfind("\n\n")
        if idx != -1 and start + idx > start:
            return start + idx + 2
        # then single newline
        idx = window.rfind("\n")
        if idx != -1 and start + idx > start:
            return start + idx + 1
        # then last space
        idx = window.rfind(" ")
        if idx != -1 and start + idx > start:
            return start + idx + 1
        return end  # hard split

    while i < L:
        end = min(i + max_chars, L)
        j = find_split(i, end)
        chunk = text[i:j]
        chunks.append(chunk)
        if j >= L:
            break
        # move with overlap
        if overlap > 0:
            i = max(j - overlap, 0)
        else:
            i = j
    return chunks

def split_row_if_needed(row: Dict[str, Any], max_chars: int, overlap: int) -> List[Dict[str, Any]]:
    """
    If row['content'] exceeds max_chars, split into multiple part-rows.
    Each part gets:
      - content := slice
      - external_id suffixed with '#part{idx}/{N}'
      - metadata augmented with parent info + part index/total
    """
    base_content = row.get("content") or ""
    parts = _greedy_chunk_text(base_content, max_chars=max_chars, overlap=overlap)
    if len(parts) == 1:
        return [row]

    total = len(parts)
    parent_ext = row.get("external_id") or ""
    parent_md = dict(row.get("metadata") or {})
    parent_md.setdefault("parting_strategy", "char_greedy")
    parent_md.setdefault("chunk_overlap", overlap)

    out_rows: List[Dict[str, Any]] = []
    for idx, part_text in enumerate(parts, start=1):
        r = dict(row)  # shallow copy
        # external id + metadata augmentation
        r["external_id"] = f"{parent_ext}#part{idx}/{total}"
        md = dict(parent_md)
        md["parent_external_id"] = parent_ext
        md["part_index"] = idx
        md["part_total"] = total
        # include parent hash (computed before knowing collection id; safe to use content + ext)
        md["parent_fingerprint"] = _hash_key([parent_ext, (base_content[:128])])
        r["metadata"] = md
        r["content"] = part_text
        # optional: we usually do NOT slice raw_code; leave as-is or drop to keep rows lighter
        # r["raw_code"] = None
        out_rows.append(r)
    return out_rows

def chunk_rows(rows: List[Dict[str, Any]], max_chars: int, overlap: int) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for r in rows:
        out.extend(split_row_if_needed(r, max_chars=max_chars, overlap=overlap))
    return out

# ------------------------------ Embedding ------------------------------
def _normalize_embedding(v: Optional[Any], dim: int) -> Optional[List[float]]:
    if v is None:
        return None
    if isinstance(v, str):
        s = v.strip()
        if s.startswith("[") and s.endswith("]"):
            s = s[1:-1]
        parts = re.split(r"[,\s]+", s)
        vec = [float(x) for x in parts if x != ""]
    elif isinstance(v, (list, tuple)):
        vec = [float(x) for x in v]
    else:
        raise ValueError("embedding must be list/tuple or string")
    if len(vec) != dim:
        raise ValueError(f"Embedding length {len(vec)} != {dim}")
    # cosine-friendly normalization
    n = math.sqrt(sum(x*x for x in vec)) or 1.0
    return [x/n for x in vec]

def embed_text_via_api(endpoint: str, text: str, timeout: int = 60) -> List[float]:
    """
    Adjust to your API shape.
    Expecting response:
      {"embedding": [float,...]}  OR  [float,...]
    """
    resp = requests.post(endpoint, json={"input": text}, timeout=timeout)
    if resp.status_code >= 400:
        raise RuntimeError(f"Embed API HTTP {resp.status_code}: {resp.text[:200]}")
    data = resp.json()
    vec = data.get("embedding", data)
    if isinstance(vec, str):
        vec = [float(x) for x in vec.replace("[","").replace("]","").split(",") if x.strip()]
    return [float(x) for x in vec]

def attach_embeddings(rows: List[Dict[str, Any]], dim: int, endpoint: Optional[str],
                      max_retries: int = 2, sleep_s: float = 0.75) -> List[Dict[str, Any]]:
    """
    Calls your embedder for each row['content'] unless endpoint is None.
    If an error occurs, leaves embedding as None (row is still inserted).
    """
    out: List[Dict[str, Any]] = []
    for r in rows:
        if endpoint is None:
            r["embedding"] = None
            out.append(r)
            continue
        content = r.get("content") or ""
        # retry small wrapper
        last_err = None
        for attempt in range(max_retries + 1):
            try:
                vec = embed_text_via_api(endpoint, content)
                r["embedding"] = _normalize_embedding(vec, dim)
                break
            except Exception as e:
                last_err = e
                if attempt < max_retries:
                    time.sleep(sleep_s * (2 ** attempt))
        else:
            # all retries failed
            sys.stderr.write(f"[warn] embed failed for external_id={r.get('external_id')}: {last_err}\n")
            r["embedding"] = None
        out.append(r)
    return out

# ------------------------------ Insert ------------------------------
def insert_rows(conn, *, collection_id: int, embedding_dim: int, rows: List[Dict[str, Any]]):
    payload = []
    for r in rows:
        if "content" not in r or not r["content"]:
            raise ValueError("Each row must contain non-empty 'content'")
        hk = r.get("hash_key") or _hash_key_for_row(collection_id, r,
                                                    part_index=(r.get("metadata") or {}).get("part_index"))
        embedding = r.get("embedding")
        if embedding is not None and len(embedding) != embedding_dim:
            raise ValueError(f"Row embedding dim {len(embedding)} != {embedding_dim}")
        payload.append((
            collection_id,
            r.get("external_id"),
            r.get("language"),
            r.get("symbol_kind"),
            bool(r.get("is_recipe", False)),
            bool(r.get("is_config", False)),
            bool(r.get("is_example", False)),
            r.get("package"),
            r.get("class_name"),
            r.get("method_name"),
            r.get("signature"),
            r.get("start_line"),
            r.get("end_line"),
            r.get("tokens", 0),
            r["content"],
            r.get("raw_code"),
            Json(r.get("metadata") or {}),
            embedding,
            hk
        ))
    with conn.cursor() as cur:
        execute_values(cur, """
          INSERT INTO kb_chunks
          (collection_id, external_id, language, symbol_kind, is_recipe, is_config, is_example,
           package, class_name, method_name, signature, start_line, end_line, tokens,
           content, raw_code, metadata, embedding, hash_key)
          VALUES %s
          ON CONFLICT (hash_key) DO NOTHING
        """, payload, page_size=150)
    conn.commit()

# ------------------------------ IO ------------------------------
def load_jsonl(path: str) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError as e:
                raise ValueError(f"{path}:{i} invalid JSON: {e}")
            rows.append(obj)
    return rows

# ------------------------------ CLI ------------------------------
def main():
    ap = argparse.ArgumentParser(description="Insert chunks with pre-embedding character chunking (pgvector).")
    ap.add_argument("--collection-name", required=True)
    ap.add_argument("--kb-type", required=True)
    ap.add_argument("--embedding-dim", type=int, default=1024)
    ap.add_argument("--description", default=None)
    ap.add_argument("--collection-metadata", default=None, help="JSON for kb_collections.metadata")
    ap.add_argument("--input-jsonl", required=True, help="Path to JSONL with rows to insert.")
    ap.add_argument("--embed-endpoint", default=None, help="HTTP endpoint for your embedder. If omitted, embeddings are NULL.")
    ap.add_argument("--max-embed-chars", type=int, default=7000, help="Max characters per embed chunk (content).")
    ap.add_argument("--overlap", type=int, default=300, help="Character overlap between consecutive chunks.")
    ap.add_argument("--no-embed", action="store_true", help="Do not call the embedding API (store NULL).")
    args = ap.parse_args()

    coll_meta = None
    if args.collection_metadata:
        coll_meta = json.loads(args.collection_metadata)

    base_rows = load_jsonl(args.input_jsonl)

    # 1) split rows by size before embedding
    chunked_rows = chunk_rows(base_rows, max_chars=args.max_embed_chars, overlap=args.overlap)

    # 2) embed (optional)
    endpoint = None if args.no_embed else args.embed_endpoint
    if not args.no_embed and not args.embed_endpoint:
        print("[info] --embed-endpoint not provided; embeddings will be NULL", file=sys.stderr)
    rows_with_vecs = attach_embeddings(chunked_rows, dim=args.embedding_dim, endpoint=endpoint)

    # 3) write to DB
    conn = connect()
    try:
        ensure_schema(conn, args.embedding_dim)
        cid = ensure_collection(conn,
                                name=args.collection_name,
                                kb_type=args.kb_type,
                                embedding_dim=args.embedding_dim,
                                description=args.description,
                                metadata=coll_meta)
        insert_rows(conn, collection_id=cid, embedding_dim=args.embedding_dim, rows=rows_with_vecs)
        print(f"[ok] collection '{args.collection_name}' (id={cid}) inserted {len(rows_with_vecs)} rows "
              f"(from {len(base_rows)} input records; chunked at {args.max_embed_chars} chars, overlap {args.overlap}).")
    finally:
        conn.close()

if __name__ == "__main__":
    main()
