#!/usr/bin/env python3
"""
kb_ingest.py  —  Create kb_* tables, upsert collection, insert chunks (idempotent)

Requires:
  pip install psycopg2-binary pgvector

Env:
  PGHOST, PGPORT, PGDATABASE, PGUSER, PGPASSWORD

Usage examples:

  # 1) Quick smoke test (demo rows; zeros embeddings)
  python kb_ingest.py \
    --collection-name code-java --kb-type code --embedding-dim 1024 --demo

  # 2) Insert from JSONL (one JSON object per line)
  python kb_ingest.py \
    --collection-name git-log --kb-type git_log --embedding-dim 1024 \
    --input-jsonl /path/to/edges_and_ledgers.jsonl
"""

from __future__ import annotations
import argparse, json, os, re, sys, hashlib
from typing import Any, Dict, List, Optional

import psycopg2
from pgvector.psycopg2 import register_vector
from psycopg2.extras import Json, execute_values

DDL = """
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;

CREATE TABLE IF NOT EXISTS kb_collections (
  id BIGSERIAL PRIMARY KEY,
  name TEXT UNIQUE NOT NULL,
  kb_type TEXT NOT NULL,                -- e.g. 'code', 'git_log', 'confluence'
  description TEXT,
  embedding_dim INT NOT NULL,
  distance TEXT NOT NULL DEFAULT 'cosine',
  metadata JSONB DEFAULT '{}'::jsonb,
  created_at TIMESTAMPTZ DEFAULT now()
);

CREATE TABLE IF NOT EXISTS kb_chunks (
  id BIGSERIAL PRIMARY KEY,
  collection_id BIGINT NOT NULL REFERENCES kb_collections(id) ON DELETE CASCADE,
  external_id TEXT,
  language TEXT,
  symbol_kind TEXT,                     -- 'code','config','upgrade','version_ledger',...
  is_recipe BOOLEAN DEFAULT FALSE,
  is_config BOOLEAN DEFAULT FALSE,
  is_example BOOLEAN DEFAULT FALSE,
  package TEXT,
  class_name TEXT,
  method_name TEXT,
  signature TEXT,
  start_line INT,
  end_line INT,
  tokens INT,
  content TEXT NOT NULL,                -- what you search over
  raw_code TEXT,                        -- optional full code
  metadata JSONB DEFAULT '{}'::jsonb,   -- your tags/keywords/etc.
  embedding VECTOR(%(dim)s),
  hash_key TEXT UNIQUE                  -- for idempotent upserts
);

CREATE INDEX IF NOT EXISTS kb_chunks_embedding_hnsw ON kb_chunks USING hnsw (embedding vector_cosine_ops);
CREATE INDEX IF NOT EXISTS kb_chunks_content_trgm   ON kb_chunks USING gin  (content gin_trgm_ops);
CREATE INDEX IF NOT EXISTS kb_chunks_collection     ON kb_chunks(collection_id);
"""

def connect():
    conn = psycopg2.connect(
        host=os.getenv("PGHOST","localhost"),
        port=int(os.getenv("PGPORT","5432")),
        dbname=os.getenv("PGDATABASE"),
        user=os.getenv("PGUSER"),
        password=os.getenv("PGPASSWORD"),
    )
    register_vector(conn)
    return conn

def ensure_schema(conn, embedding_dim: int):
    with conn.cursor() as cur:
        cur.execute(DDL, {"dim": embedding_dim})
    conn.commit()

def ensure_collection(conn, *, name: str, kb_type: str, embedding_dim: int,
                      description: Optional[str] = None,
                      metadata: Optional[Dict[str, Any]] = None) -> int:
    metadata = metadata or {}
    with conn.cursor() as cur:
        cur.execute("""
            INSERT INTO kb_collections(name, kb_type, description, embedding_dim, metadata)
            VALUES (%s,%s,%s,%s,%s)
            ON CONFLICT (name) DO NOTHING;
        """, (name, kb_type, description, embedding_dim, Json(metadata)))
        cur.execute("SELECT id, embedding_dim FROM kb_collections WHERE name=%s", (name,))
        row = cur.fetchone()
        if not row:
            raise RuntimeError("Failed to read/create collection")
        cid, dim = row
        if dim != embedding_dim:
            raise ValueError(f"Collection '{name}' embedding_dim={dim} != expected {embedding_dim}. "
                             f"Use a new collection or re-create with the desired dimension.")
        return cid

def _hash_key_for_row(collection_id: int, r: Dict[str, Any]) -> str:
    # Stable hash: collection + external_id + symbol_kind + versions/commit + first 128 chars of content
    md = r.get("metadata") or {}
    fields = [
        str(collection_id),
        r.get("external_id") or "",
        r.get("symbol_kind") or "",
        md.get("commit",""),
        (md.get("from_version","") + "->" + md.get("to_version","")) if (md.get("from_version") or md.get("to_version")) else "",
        (r.get("content") or "")[:128],
    ]
    return hashlib.sha256("|".join(fields).encode("utf-8")).hexdigest()

def _normalize_embedding(v: Optional[Any], dim: int) -> List[float]:
    if v is None:
        return [0.0]*dim
    if isinstance(v, str):
        s = v.strip()
        if s.startswith("[") and s.endswith("]"):
            s = s[1:-1]
        parts = re.split(r"[,\s]+", s)
        vec = [float(x) for x in parts if x != ""]
    elif isinstance(v, (list, tuple)):
        vec = [float(x) for x in v]
    else:
        raise ValueError("embedding must be list/tuple or string")
    if len(vec) != dim:
        raise ValueError(f"Embedding length {len(vec)} != {dim}")
    # (Optional) L2-normalize — depends on how your retrieval expects vectors
    # Here we keep as-is; uncomment to normalize:
    # import math
    # n = math.sqrt(sum(x*x for x in vec)) or 1.0
    # vec = [x/n for x in vec]
    return vec

def insert_chunks(conn, *, collection_id: int, embedding_dim: int, rows: List[Dict[str, Any]]):
    payload = []
    for r in rows:
        if "content" not in r or not r["content"]:
            raise ValueError("Each row must contain non-empty 'content'")
        embedding = _normalize_embedding(r.get("embedding"), embedding_dim)
        hk = r.get("hash_key") or _hash_key_for_row(collection_id, r)
        payload.append((
            collection_id,
            r.get("external_id"),
            r.get("language"),
            r.get("symbol_kind"),
            bool(r.get("is_recipe", False)),
            bool(r.get("is_config", False)),
            bool(r.get("is_example", False)),
            r.get("package"),
            r.get("class_name"),
            r.get("method_name"),
            r.get("signature"),
            r.get("start_line"),
            r.get("end_line"),
            r.get("tokens", 0),
            r["content"],
            r.get("raw_code"),
            Json(r.get("metadata") or {}),
            embedding,
            hk
        ))

    with conn.cursor() as cur:
        execute_values(cur, """
          INSERT INTO kb_chunks
          (collection_id, external_id, language, symbol_kind, is_recipe, is_config, is_example,
           package, class_name, method_name, signature, start_line, end_line, tokens,
           content, raw_code, metadata, embedding, hash_key)
          VALUES %s
          ON CONFLICT (hash_key) DO NOTHING
        """, payload, page_size=200)
    conn.commit()

def load_jsonl(path: str) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                rows.append(json.loads(line))
            except json.JSONDecodeError as e:
                raise ValueError(f"Invalid JSON on line {i}: {e}")
    return rows

def demo_rows() -> List[Dict[str, Any]]:
    # Two tiny rows to smoke-test inserts
    return [
        {
            "external_id": "src/main/java/com/acme/logging/LoggerConfig.java",
            "language": "java",
            "symbol_kind": "code",
            "is_recipe": True,
            "content": (
                "How to configure SLF4J + Logback in Spring Boot.\n"
                "Steps:\n1) Add ch.qos.logback:logback-classic 3.0.15\n"
                "2) Set logging.level.root=INFO\n"
            ),
            "raw_code": "package com.acme.logging; public class LoggerConfig {}",
            "metadata": {
                "framework_folder": "logging-framework",
                "tags": ["logging","java","recipe"],
                "keywords": ["slf4j","logback","spring-boot","logging.level.root"],
            },
            "embedding": None  # zeros
        },
        {
            "external_id": "git:commit/abc123",
            "language": "text",
            "symbol_kind": "upgrade",
            "is_recipe": True,
            "content": (
                "[Upgrade]\nFramework: chassis\nCommit: abc123\nDate: 2025-01-01\n"
                "Change: com.acme:chassis-core 3.0.0 → 3.0.15 (minor)\n\nDiff:\n+ <version>3.0.15</version>"
            ),
            "metadata": {
                "framework_folder": "chassis",
                "group": "com.acme",
                "artifact": "chassis-core",
                "from_version": "3.0.0",
                "to_version": "3.0.15",
                "semver_delta": "minor",
                "tags": ["upgrade"],
                "keywords": ["chassis","3.0.0","3.0.15"]
            },
            "embedding": None  # zeros
        }
    ]

def main():
    ap = argparse.ArgumentParser(description="Insert chunks into kb_collections/kb_chunks (pgvector).")
    ap.add_argument("--collection-name", required=True, help="Unique name, e.g., code-java, git-log, confluence.")
    ap.add_argument("--kb-type", required=True, help="Type label, e.g., code, git_log, confluence.")
    ap.add_argument("--embedding-dim", type=int, default=1024, help="Vector dimension for this collection.")
    ap.add_argument("--description", default=None, help="Optional collection description.")
    ap.add_argument("--collection-metadata", default=None, help="JSON for kb_collections.metadata (optional).")
    ap.add_argument("--input-jsonl", default=None, help="Path to JSONL with rows to insert.")
    ap.add_argument("--demo", action="store_true", help="Insert two demo rows (for smoke test).")
    args = ap.parse_args()

    if not args.input_jsonl and not args.demo:
        print("Provide --input-jsonl or --demo", file=sys.stderr)
        sys.exit(2)

    coll_meta = None
    if args.collection_metadata:
        try:
            coll_meta = json.loads(args.collection_metadata)
        except Exception as e:
            print(f"Invalid --collection-metadata JSON: {e}", file=sys.stderr)
            sys.exit(2)

    rows = demo_rows() if args.demo else load_jsonl(args.input_jsonl)

    conn = connect()
    try:
        ensure_schema(conn, args.embedding_dim)
        cid = ensure_collection(conn,
                                name=args.collection_name,
                                kb_type=args.kb_type,
                                embedding_dim=args.embedding_dim,
                                description=args.description,
                                metadata=coll_meta)
        insert_chunks(conn, collection_id=cid, embedding_dim=args.embedding_dim, rows=rows)
        print(f"[ok] collection '{args.collection_name}' (id={cid}) inserted {len(rows)} rows.")
    finally:
        conn.close()

if __name__ == "__main__":
    main()
